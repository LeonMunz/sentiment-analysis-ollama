{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import gensim\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import spacy\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "from functions import data_prep\n",
    "from functions import merge_df_cols"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-25T17:21:46.006246800Z",
     "start_time": "2024-06-25T17:21:38.191488600Z"
    }
   },
   "id": "e6b64677ec2e28dd",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lmunz\\AppData\\Local\\Temp\\ipykernel_664\\569604983.py:10: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_eval_c['created_at'] = pd.to_datetime(df_eval_c['created_at'], errors='coerce')\n"
     ]
    }
   ],
   "source": [
    "# Prepare dfs\n",
    "df = pd.read_csv('data/pepper_dump_19_06_2024.csv')\n",
    "\n",
    "# BioPic Model\n",
    "df_eval_c = pd.read_csv('data/pepper_biopyschosocialenv_manually_labeled_responses_redacted.csv')\n",
    "\n",
    "# Sentiment\n",
    "#df_eval_c = pd.read_csv('data/peppers_manually_labeled_response_redacted.csv')\n",
    "\n",
    "df_eval_c['created_at'] = pd.to_datetime(df_eval_c['created_at'], errors='coerce')\n",
    "df_eval_c['created_at'] = df_eval_c['created_at'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# merge all cols to df_eval_c on FsDaily_id\n",
    "additional_columns = [col for col in df.columns if col not in df_eval_c.columns]\n",
    "df_eval_c = df_eval_c.merge(df[['FsDaily_id'] + additional_columns], on='FsDaily_id', how='inner')\n",
    "\n",
    "# Apply preprocessing\n",
    "df = data_prep(df)\n",
    "df_eval = data_prep(df_eval_c)\n",
    "\n",
    "# Merge df\n",
    "data_df = merge_df_cols(df_eval)\n",
    "#data_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-25T17:25:45.732601700Z",
     "start_time": "2024-06-25T17:25:45.662604100Z"
    }
   },
   "id": "c01743ae664ec5a1",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#data_df['manual_label'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-25T17:25:28.799254800Z",
     "start_time": "2024-06-25T17:25:28.753255300Z"
    }
   },
   "id": "d44c50be6d206b07",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "data_df.drop_duplicates(subset=[\"text\"], inplace=True)\n",
    "#data_df['manual_label'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-25T17:25:32.684332700Z",
     "start_time": "2024-06-25T17:25:32.671333500Z"
    }
   },
   "id": "36a6208733001bb3",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lmunz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lmunz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "C:\\ProgramData\\anaconda3\\envs\\train_model_new\\lib\\site-packages\\spacy\\util.py:910: UserWarning: [W095] Model 'de_core_news_md' (3.1.0) was trained with spaCy v3.1.0 and may not be 100% compatible with the current version (3.7.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "# Vor dem ersten Gebrauch musst du die Stopwörter und die Punktuation herunterladen\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Laden des Spacy-Sprachmodells für Deutsch\n",
    "nlp = spacy.load('de_core_news_md')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text, language='german')\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "\n",
    "    stop_words = set(stopwords.words('german'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    doc = nlp(\" \".join(tokens))\n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "    \n",
    "    preprocessed_text = ' '.join(lemmas)\n",
    "\n",
    "    return preprocessed_text\n",
    "\n",
    "\n",
    "data_df['text'] = data_df['text'].apply(preprocess_text)\n",
    "#data_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-25T17:25:26.213172Z",
     "start_time": "2024-06-25T17:25:19.854170700Z"
    }
   },
   "id": "7f0cf607898974d6",
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e64685da1e1b163a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# Naive Bayes Classifier for Multinomial Models"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3fc9b54a7713f20c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "X = data_df.text\n",
    "y = data_df.manual_label\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-25T17:21:52.796272700Z",
     "start_time": "2024-06-25T17:21:52.770274600Z"
    }
   },
   "id": "abec6e73cea53996",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "my_tags = ['other','physical','mental','environmental','social']\n",
    "# plt.figure(figsize=(10,4))\n",
    "# data_df.manual_label.value_counts().plot(kind='bar');"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-25T17:28:07.637568300Z",
     "start_time": "2024-06-25T17:28:07.620563800Z"
    }
   },
   "id": "93e6379d40a8a383",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.6602870813397129\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        other       1.00      0.21      0.35        14\n",
      "     physical       0.00      0.00      0.00         7\n",
      "       mental       0.61      0.98      0.76       109\n",
      "environmental       0.89      0.44      0.59        39\n",
      "       social       0.85      0.28      0.42        40\n",
      "\n",
      "     accuracy                           0.66       209\n",
      "    macro avg       0.67      0.38      0.42       209\n",
      " weighted avg       0.72      0.66      0.61       209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\envs\\train_model_new\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\anaconda3\\envs\\train_model_new\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\anaconda3\\envs\\train_model_new\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "nb = Pipeline([('vect', CountVectorizer()),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', MultinomialNB()),\n",
    "              ])\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred = nb.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=my_tags))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-25T17:21:53.211272Z",
     "start_time": "2024-06-25T17:21:53.026250900Z"
    }
   },
   "id": "8fe0151a9ff890e3",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# Linear Support Vector Machine"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba7424c17df41c23"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.7559808612440191\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        other       0.92      0.79      0.85        14\n",
      "     physical       1.00      0.29      0.44         7\n",
      "       mental       0.76      0.90      0.82       109\n",
      "environmental       0.72      0.59      0.65        39\n",
      "       social       0.71      0.60      0.65        40\n",
      "\n",
      "     accuracy                           0.76       209\n",
      "    macro avg       0.82      0.63      0.68       209\n",
      " weighted avg       0.76      0.76      0.75       209\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)),\n",
    "               ])\n",
    "sgd.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = sgd.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=my_tags))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-25T17:21:53.263246600Z",
     "start_time": "2024-06-25T17:21:53.072248900Z"
    }
   },
   "id": "2b25a1f5ca73b118",
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# Logistic Regression"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3baff7d0835a9542"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.7703349282296651\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        other       0.92      0.79      0.85        14\n",
      "     physical       1.00      0.29      0.44         7\n",
      "       mental       0.75      0.94      0.83       109\n",
      "environmental       0.84      0.54      0.66        39\n",
      "       social       0.74      0.62      0.68        40\n",
      "\n",
      "     accuracy                           0.77       209\n",
      "    macro avg       0.85      0.63      0.69       209\n",
      " weighted avg       0.78      0.77      0.76       209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\envs\\train_model_new\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', LogisticRegression(n_jobs=1, C=1e5)),\n",
    "               ])\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=my_tags))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-25T17:21:53.265248Z",
     "start_time": "2024-06-25T17:21:53.119249700Z"
    }
   },
   "id": "374e63cb6c167003",
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# Word2vec and Logistic Regression Fasttext embeddings"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e025a1cb8d1029e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Download german model:\n",
    "- https://fasttext.cc/docs/en/crawl-vectors.html"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d71f4021f93b030e"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'models/cc.de.300.vec.gz'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[11], line 5\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# Korrektes Laden des Modells\u001B[39;00m\n\u001B[0;32m      4\u001B[0m model_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodels/cc.de.300.vec.gz\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m----> 5\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mload_facebook_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m# Teste das Modell\u001B[39;00m\n\u001B[0;32m      7\u001B[0m word_vector \u001B[38;5;241m=\u001B[39m model[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mHaus\u001B[39m\u001B[38;5;124m'\u001B[39m]  \u001B[38;5;66;03m# Erhalte den Vektor für das Wort 'Haus'\u001B[39;00m\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\train_model_new\\lib\\site-packages\\gensim\\models\\fasttext.py:728\u001B[0m, in \u001B[0;36mload_facebook_model\u001B[1;34m(path, encoding)\u001B[0m\n\u001B[0;32m    666\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload_facebook_model\u001B[39m(path, encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[0;32m    667\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Load the model from Facebook's native fasttext `.bin` output file.\u001B[39;00m\n\u001B[0;32m    668\u001B[0m \n\u001B[0;32m    669\u001B[0m \u001B[38;5;124;03m    Notes\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    726\u001B[0m \n\u001B[0;32m    727\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 728\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_load_fasttext_format\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfull_model\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\train_model_new\\lib\\site-packages\\gensim\\models\\fasttext.py:807\u001B[0m, in \u001B[0;36m_load_fasttext_format\u001B[1;34m(model_file, encoding, full_model)\u001B[0m\n\u001B[0;32m    788\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_load_fasttext_format\u001B[39m(model_file, encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m'\u001B[39m, full_model\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[0;32m    789\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Load the input-hidden weight matrix from Facebook's native fasttext `.bin` output files.\u001B[39;00m\n\u001B[0;32m    790\u001B[0m \n\u001B[0;32m    791\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    805\u001B[0m \n\u001B[0;32m    806\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 807\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43mutils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_file\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m fin:\n\u001B[0;32m    808\u001B[0m         m \u001B[38;5;241m=\u001B[39m gensim\u001B[38;5;241m.\u001B[39mmodels\u001B[38;5;241m.\u001B[39m_fasttext_bin\u001B[38;5;241m.\u001B[39mload(fin, encoding\u001B[38;5;241m=\u001B[39mencoding, full_model\u001B[38;5;241m=\u001B[39mfull_model)\n\u001B[0;32m    810\u001B[0m     model \u001B[38;5;241m=\u001B[39m FastText(\n\u001B[0;32m    811\u001B[0m         vector_size\u001B[38;5;241m=\u001B[39mm\u001B[38;5;241m.\u001B[39mdim,\n\u001B[0;32m    812\u001B[0m         window\u001B[38;5;241m=\u001B[39mm\u001B[38;5;241m.\u001B[39mws,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    821\u001B[0m         max_n\u001B[38;5;241m=\u001B[39mm\u001B[38;5;241m.\u001B[39mmaxn,\n\u001B[0;32m    822\u001B[0m     )\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\train_model_new\\lib\\site-packages\\smart_open\\smart_open_lib.py:235\u001B[0m, in \u001B[0;36mopen\u001B[1;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, compression, transport_params)\u001B[0m\n\u001B[0;32m    232\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m ve:\n\u001B[0;32m    233\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m(ve\u001B[38;5;241m.\u001B[39margs[\u001B[38;5;241m0\u001B[39m])\n\u001B[1;32m--> 235\u001B[0m binary \u001B[38;5;241m=\u001B[39m \u001B[43m_open_binary_stream\u001B[49m\u001B[43m(\u001B[49m\u001B[43muri\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbinary_mode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtransport_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    236\u001B[0m decompressed \u001B[38;5;241m=\u001B[39m so_compression\u001B[38;5;241m.\u001B[39mcompression_wrapper(binary, binary_mode, compression)\n\u001B[0;32m    238\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode \u001B[38;5;129;01mor\u001B[39;00m explicit_encoding \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\train_model_new\\lib\\site-packages\\smart_open\\smart_open_lib.py:398\u001B[0m, in \u001B[0;36m_open_binary_stream\u001B[1;34m(uri, mode, transport_params)\u001B[0m\n\u001B[0;32m    396\u001B[0m scheme \u001B[38;5;241m=\u001B[39m _sniff_scheme(uri)\n\u001B[0;32m    397\u001B[0m submodule \u001B[38;5;241m=\u001B[39m transport\u001B[38;5;241m.\u001B[39mget_transport(scheme)\n\u001B[1;32m--> 398\u001B[0m fobj \u001B[38;5;241m=\u001B[39m \u001B[43msubmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen_uri\u001B[49m\u001B[43m(\u001B[49m\u001B[43muri\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtransport_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    399\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(fobj, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[0;32m    400\u001B[0m     fobj\u001B[38;5;241m.\u001B[39mname \u001B[38;5;241m=\u001B[39m uri\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\envs\\train_model_new\\lib\\site-packages\\smart_open\\local_file.py:34\u001B[0m, in \u001B[0;36mopen_uri\u001B[1;34m(uri_as_string, mode, transport_params)\u001B[0m\n\u001B[0;32m     32\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mopen_uri\u001B[39m(uri_as_string, mode, transport_params):\n\u001B[0;32m     33\u001B[0m     parsed_uri \u001B[38;5;241m=\u001B[39m parse_uri(uri_as_string)\n\u001B[1;32m---> 34\u001B[0m     fobj \u001B[38;5;241m=\u001B[39m \u001B[43mio\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparsed_uri\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43muri_path\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     35\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fobj\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'models/cc.de.300.vec.gz'"
     ]
    }
   ],
   "source": [
    "\n",
    "from gensim.models.fasttext import load_facebook_model\n",
    "\n",
    "model_path = 'models/cc.de.300.vec.gz'\n",
    "model = load_facebook_model(model_path)\n",
    "\n",
    "word_vector = model['Haus']\n",
    "print(word_vector)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-25T17:21:54.183249400Z",
     "start_time": "2024-06-25T17:21:53.216250300Z"
    }
   },
   "id": "6e1eaee6dbf2901b",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim\n",
    "import logging\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import nltk\n",
    "\n",
    "def word_averaging(wv, words):\n",
    "    all_words, mean = set(), []\n",
    "    \n",
    "    for word in words:\n",
    "        if isinstance(word, np.ndarray):\n",
    "            mean.append(word)\n",
    "        elif word in wv.key_to_index:\n",
    "            mean.append(wv[word])\n",
    "            all_words.add(wv.key_to_index[word])\n",
    "\n",
    "    if not mean:\n",
    "        logging.warning(\"cannot compute similarity with no input %s\", words)\n",
    "        return np.zeros(wv.vector_size,)\n",
    "\n",
    "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "    return mean\n",
    "\n",
    "def word_averaging_list(wv, text_list):\n",
    "    return np.vstack([word_averaging(wv, post) for post in text_list])\n",
    "\n",
    "def w2v_tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text, language='german'):\n",
    "        for word in nltk.word_tokenize(sent, language='german'):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "train, test = train_test_split(data_df, test_size=0.3, random_state=42)\n",
    "\n",
    "test_tokenized = test.apply(lambda r: w2v_tokenize_text(r['text']), axis=1).values\n",
    "train_tokenized = train.apply(lambda r: w2v_tokenize_text(r['text']), axis=1).values\n",
    "\n",
    "X_train_word_average = word_averaging_list(wv, train_tokenized)\n",
    "X_test_word_average = word_averaging_list(wv, test_tokenized)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
    "logreg = logreg.fit(X_train_word_average, train['manual_label'])\n",
    "y_pred = logreg.predict(X_test_word_average)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, test['manual_label']))\n",
    "print(classification_report(test['manual_label'], y_pred, target_names=my_tags))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-25T17:21:54.204247700Z",
     "start_time": "2024-06-25T17:21:54.187255400Z"
    }
   },
   "id": "475fe46cf786524c",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
